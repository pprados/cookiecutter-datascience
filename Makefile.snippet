#!/usr/bin/env make

# SNIPPET Le shebang précédant permet de creer des alias des cibles du Makefile.
# Il faut que le Makefile soit executable
# 	chmod u+x Makefile
# 	git update-index --chmod=+x Makefile
# Puis, par exemple
# 	ln -s Makefile configure
# 	ln -s Makefile test
# 	ln -s Makefile train
# 	./configure		# Execute make configure
# 	./test 			# Execute make test
#   ./train 		# Train the model
# Attention, il n'est pas possible de passer les paramètres aux scripts

# ---------------------------------------------------------------------------------------
# SNIPPET pour changer le mode de gestion du Makefile.
# Avec ces trois paramètres, toutes les lignes d'une recette sont invoquées dans le même shell.
# Ainsi, il n'est pas nécessaire d'ajouter des '&&' ou des '\' pour regrouper les lignes.
# Comme Make affiche l'intégralité du block de la recette avant de l'exécuter, il n'est
# pas toujours facile de savoir quel est la ligne en échec.
# Je vous conseille dans ce cas d'ajouter au début de la recette 'set -x'
# Attention : il faut une version > 4 de  `make` (`make -v`).
# Les versions CentOS d'Amazone ont une version 3.82.
# Utilisez `conda install -n $(VENV_AWS) make>=4 -y`
# WARNING: Use make >4.0
SHELL=/bin/bash
.SHELLFLAGS = -e -c
.ONESHELL:

# ---------------------------------------------------------------------------------------
# SNIPPET pour détecter l'OS d'exécution.
ifeq ($(OS),Windows_NT)
    OS := Windows
else
    OS := $(shell sh -c 'uname 2>/dev/null || echo Unknown')
endif

# ---------------------------------------------------------------------------------------
# SNIPPET pour détecter la présence d'un GPU afin de modifier le nom du projet
# et ses dépendances si nécessaire.
ifdef GPU
USE_GPU:=$(shell [[ "$$GPU" == yes ]] && echo "-gpu")
else ifneq ("$(wildcard /proc/driver/nvidia)","")
USE_GPU:=-gpu
else ifdef CUDA_PATH
USE_GPU:=-gpu
endif

# ---------------------------------------------------------------------------------------
# SNIPPET pour identifier le nombre de processeur
NB_CORE:=$(shell grep -c '^processor' /proc/cpuinfo)

# ---------------------------------------------------------------------------------------
# SNIPPET pour pouvoir lancer un browser avec un fichier local
define BROWSER
	python -c '
	import os, sys, webbrowser
	from urllib.request import pathname2url

	webbrowser.open("file://" + pathname2url(os.path.abspath(sys.argv[1])), autoraise=True)
	sys.exit(0)
	'
endef

# ---------------------------------------------------------------------------------------
# SNIPPET pour supprimer le parallelisme pour certaines cibles
# par exemple pour release
#ifneq ($(filter release,$(MAKECMDGOALS)),)
#.NOTPARALLEL:
#endif
#

# ---------------------------------------------------------------------------------------
# SNIPPET pour récupérer les séquences de caractères pour les couleurs
# A utiliser avec un
# echo -e "Use '$(cyan)make$(normal)' ..."
# Si vous n'utilisez pas ce snippet, les variables de couleurs non initialisés
# sont simplement ignorées.
ifneq ($(TERM),)
normal:=$(shell tput sgr0)
bold:=$(shell tput bold)
red:=$(shell tput setaf 1)
green:=$(shell tput setaf 2)
yellow:=$(shell tput setaf 3)
blue:=$(shell tput setaf 4)
purple:=$(shell tput setaf 5)
cyan:=$(shell tput setaf 6)
white:=$(shell tput setaf 7)
gray:=$(shell tput setaf 8)
endif

# ---------------------------------------------------------------------------------------
# SNIPPET pour gérer le projet, le virtualenv conda et le kernel
# Par convention, les noms du projet, de l'environnement Conda ou le Kernel Jupyter
# correspondent au nom du répertoire du projet.
# Il est possible de modifier cela en valorisant les variables VENV et/ou KERNEL
# avant le lancement du Makefile (`VENV=cntk_p36 make`)
PRJ:=$(shell basename $(shell pwd))
VENV ?= $(PRJ)
KERNEL ?=$(VENV)
PRJ_PACKAGE:=$(PRJ)$(USE_GPU)
PYTHON_VERSION:=3.6

# Data directory (can be in other place, in VM for example)
export DATA?=data

# Use DVC bucket. May be :
# DVC_BUCKET?=s3://$(PRJ)
# DVC_BUCKET?=gs://$(PRJ)
# DVC_BUCKET?=ssh://user@example.com:
# DVC_BUCKET?=hdfs://user@example.com
DVC_BUCKET?=~/.dvc
PYTHON_SRC=$(shell find bda_project -type f -iname '*.py' | grep -v __pycache__)
PYTHON_TST=$(shell find tests -type f -iname '*.py' | grep -v __pycache__)
# ---------------------------------------------------------------------------------------
# SNIPPET pour reconstruire tous les chemins importants permettant
# de gérer correctement les dépendances des modules sous Conda.
# Cela servira à gérer automatiquement les environnements.
# Pour que cela fonctionne, il faut avoir un environement Conda actif,
# identifié par la variable CONDA_PREFIX (c'est généralement le cas).
CONDA_BASE:=$(shell conda info --base)
CONDA_PACKAGE:=$(CONDA_PREFIX)/lib/python$(PYTHON_VERSION)/site-packages
CONDA_PYTHON:=$(CONDA_PREFIX)/bin/python
PIP_PACKAGE:=$(CONDA_PACKAGE)/$(PRJ_PACKAGE).egg-link
JUPYTER_DATA_DIR:=$(shell jupyter --data-dir 2>/dev/null || echo "~/.local/share/jupyter")

# ---------------------------------------------------------------------------------------
# SNIPPET pour ajouter des repositories complémentaires à PIP.
# A utiliser avec par exemple
# pip $(EXTRA_INDEX) install ...
EXTRA_INDEX:=--extra-index-url=https://pypi.anaconda.org/octo

# ---------------------------------------------------------------------------------------
# SNIPPET pour gérer automatiquement l'aide du Makefile.
# Il faut utiliser des commentaires commençant par '##' précédant la ligne des recettes,
# pour une production automatique de l'aide.
.PHONY: help
.DEFAULT: help

## Print all majors target
help:
	@echo "$(bold)Available rules:$(normal)"
	@echo
	@sed -n -e "/^## / { \
		h; \
		s/.*//; \
		:doc" \
		-e "H; \
		n; \
		s/^## //; \
		t doc" \
		-e "s/:.*//; \
		G; \
		s/\\n## /---/; \
		s/\\n/ /g; \
		p; \
	}" ${MAKEFILE_LIST} \
	| LC_ALL='C' sort --ignore-case \
	| awk -F '---' \
		-v ncol=$$(tput cols) \
		-v indent=20 \
		-v col_on="$$(tput setaf 6)" \
		-v col_off="$$(tput sgr0)" \
	'{ \
		printf "%s%*s%s ", col_on, -indent, $$1, col_off; \
		n = split($$2, words, " "); \
		line_length = ncol - indent; \
		for (i = 1; i <= n; i++) { \
			line_length -= length(words[i]) + 1; \
			if (line_length <= 0) { \
				line_length = ncol - indent - length(words[i]) - 1; \
				printf "\n%*s ", -indent, " "; \
			} \
			printf "%s ", words[i]; \
		} \
		printf "\n"; \
	}' \
	| more $(shell test $(shell uname) = Darwin && echo '--no-init --raw-control-chars')

	echo -e "Use '$(cyan)make -jn ...$(normal)' for Parallel run"
	echo -e "Use '$(cyan)make -B ...$(normal)' to force the target"
	echo -e "Use '$(cyan)make -n ...$(normal)' to simulate the build"

# ---------------------------------------------------------------------------------------
# SNIPPET pour affichier la valeur d'une variable d'environnement
# tel quelle est vue par le Makefile. Par exemple `make dump-CONDA_PACKAGE`
.PHONY: dump-*
dump-%:
	@if [ "${${*}}" = "" ]; then
		echo "Environment variable $* is not set";
		exit 1;
	else
		echo "$*=${${*}}";
	fi

# ---------------------------------------------------------------------------------------
# SNIPPET pour gérer les Notebooks avec GIT.
# Les recettes suivantes s'assure que git est bien initialisé
# et ajoute des recettes pour les fichiers *.ipynb
# et eventuellement pour les fichiers *.csv.
#
# Pour cela, un fichier .gitattribute est maintenu à jour.
# Les recettes pour les notebooks se chargent de les nettoyer avant de les commiter.
# Pour cela, elles appliquent `jupyter nb-convert` à la volée. Ainsi, les comparaisons
# de version ne sont plus parasités par les data.
#
# Les scripts pour les CSV utilisent le composant `daff` (pip install daff)
# pour comparer plus efficacement les évolutions des fichiers csv.
# Un `git diff toto.csv` est plus clair.

# S'assure de la présence de git (util en cas de synchronisation sur le cloud par exemple,
# après avoir exclus le répertoire .git (cf. ssh-ec2)
.git:
	@git init -q
	git commit --allow-empty -m "Create project bda_project"

# Règle technique importante, invoquées lors du `git commit` d'un fichier *.ipynb via
# le paramètrage de `.gitattributes`.
.PHONY: pipe_clear_jupyter_output
pipe_clear_jupyter_output:
	jupyter nb-convert --to notebook --ClearOutputPreprocessor.enabled=True <(cat <&0) --stdout 2>/dev/null


# Règle qui ajoute la validation du project avant un push sur la branche master.
# Elle ajoute un hook git pour invoquer `make validate` avant de pusher. En cas
# d'erreur, le push n'est pas possible.
# Pour forcer le push, malgres des erreurs lors de l'exécution de 'make validate'
# utilisez 'FORCE=y git push'.
# Pour supprimer ce comportement, il faut modifier le fichier .git/hooks/pre-push
# et supprimer la règle du Makefile, ou bien,
# utiliser un fichier vide 'echo ''> .git/hooks/pre-push'
.git/hooks/pre-push: | .git
	@# Add a hook to validate the project before a git push
	cat >>.git/hooks/pre-push <<PRE-PUSH
	#!/usr/bin/env sh
	# Validate the project before a push
	if test -t 1; then
		ncolors=$$(tput colors)
		if test -n "\$$ncolors" && test \$$ncolors -ge 8; then
			normal="\$$(tput sgr0)"
			red="\$$(tput setaf 1)"
	        green="\$$(tput setaf 2)"
			yellow="\$$(tput setaf 3)"
		fi
	fi
	branch="\$$(git branch | grep \* | cut -d ' ' -f2)"
	if [ "\$${branch}" = "master" ] && [ "\$${FORCE}" != y ] ; then
		printf "\$${green}Validate the project before push the commit... (\$${yellow}make validate\$${green})\$${normal}\n"
		make validate
		ERR=\$$?
		if [ \$${ERR} -ne 0 ] ; then
			printf "\$${red}'\$${yellow}make validate\$${red}' failed before git push.\$${normal}\n"
			printf "Use \$${yellow}FORCE=y git push\$${normal} to force.\n"
			exit \$${ERR}
		fi
	fi
	PRE-PUSH
	chmod +x .git/hooks/pre-push

# Init git configuration
.gitattributes: | .git .git/hooks/pre-push .dvc # Configure git
	@git config --local core.autocrlf input
	# Set tabulation to 4 when use 'git diff'
	@git config --local core.page 'less -x4'

ifeq ($(shell which jupyter >/dev/null ; echo "$$?"),0)
	# Add rules to manage the output data of notebooks
	@git config --local filter.dropoutput_jupyter.clean "make --silent pipe_clear_jupyter_output"
	@git config --local filter.dropoutput_jupyter.smudge cat
	@[ -e .gitattributes ] && grep -v dropoutput_jupyter .gitattributes >.gitattributes.new 2>/dev/null || true
	@[ -e .gitattributes.new ] && mv .gitattributes.new .gitattributes || true
	@echo "*.ipynb filter=dropoutput_jupyter diff=dropoutput_jupyter -text" >>.gitattributes
endif

ifeq ($(shell which daff >/dev/null ; echo "$$?"),0)
	# Add rules to manage diff with daff for CSV file
	@git config --local diff.daff-csv.command "daff.py diff --git"
	@git config --local merge.daff-csv.name "daff.py tabular merge"
	@git config --local merge.daff-csv.driver "daff.py merge --output %A %O %A %B"
	@[ -e .gitattributes ] && grep -v daff-csv .gitattributes >.gitattributes.new 2>/dev/null
	@[ -e .gitattributes.new ] && mv .gitattributes.new .gitattributes
	@echo "*.[tc]sv diff=daff-csv merge=daff-csv -text" >>.gitattributes
endif

ifeq ($(shell which dvc >/dev/null ; echo "$$?"),0)
	# Add DVC/Git integration
	@dvc install
endif

# ---------------------------------------------------------------------------------------
# SNIPPET pour vérifier la présence d'un environnement Conda conforme
# avant le lancement d'un traitement.
# Il faut ajouter $(VALIDATE_VENV) dans les recettes
# et choisir la version à appliquer.
# Soit :
# - CHECK_VENV pour vérifier l'activation d'un VENV avant de commencer
# - ACTIVATE_VENV pour activer le VENV avant le traitement
# Pour cela, sélectionnez la version de VALIDATE_VENV qui vous convient.
# Attention, toute les règles proposées ne sont pas compatible avec le mode ACTIVATE_VENV
CHECK_VENV=@if [[ "base" == "$(CONDA_DEFAULT_ENV)" ]] || [[ -z "$(CONDA_DEFAULT_ENV)" ]] ; \
  then ( echo -e "$(green)Use: $(cyan)conda activate $(VENV)$(green) before using 'make'$(normal)"; exit 1 ) ; fi

ACTIVATE_VENV=source $(CONDA_BASE)/etc/profile.d/conda.sh && conda activate $(VENV)
DEACTIVATE_VENV=source $(CONDA_BASE)/etc/profile.d/conda.sh && conda deactivate

VALIDATE_VENV=$(CHECK_VENV)
#VALIDATE_VENV=$(ACTIVATE_VENV)

# ---------------------------------------------------------------------------------------
# SNIPPET pour gérer correctement toute les dépendences python du projet.
# La cible `requirements` se charge de gérer toutes les dépendences
# d'un projet Python. Dans le SNIPPET présenté, il y a de quoi gérer :
# - les dépendances PIP
# - l'import de données pour spacy
# - l'import de données pour nltk
# - la gestion d'un kernel pour Jupyter
#
# Il suffit, dans les autres de règles, d'ajouter la dépendances sur `$(REQUIREMENTS)`
# pour qu'un simple `make test` garantie la mise à jour de l'environnement avant
# le lancement des tests par exemple.
#
# Pour cela, il faut indiquer dans le fichier 'setup.py', toutes les dépendances
# de run et de test (voir le modèle de `setup.py` proposé)

# All dependencies of the project must be here
.PHONY: requirements dependencies
REQUIREMENTS=$(PIP_PACKAGE) \
	$(NLTK_DATABASE) $(SPACY_DATABASE) \
	.gitattributes
requirements: $(REQUIREMENTS)
dependencies: requirements

# Rule to check the good installation of python in Conda venv
$(CONDA_PYTHON):
	@$(VALIDATE_VENV)
	conda install -q "python=$(PYTHON_VERSION).*" -y -q

# Rule to update the current venv, with the dependencies describe in `setup.py`
$(PIP_PACKAGE): $(CONDA_PYTHON) setup.py | .git # Install pip dependencies
	@$(VALIDATE_VENV)
	pip install -q $(EXTRA_INDEX) -e '.[dev]' | grep -v 'already satisfied' || true
	@touch $(PIP_PACKAGE)

# ---------------------------------------------------------------------------------------
# SNIPPET pour gérer les kernels Jupyter
# Intall a Jupyter kernel
$(JUPYTER_DATA_DIR)/kernels/$(KERNEL): $(REQUIREMENTS)
	@$(VALIDATE_VENV)
	python -O -m ipykernel install --user --name $(KERNEL)

# Remove the kernel
.PHONY: remove-kernel
remove-kernel:
	@echo y | jupyter kernelspec uninstall $(KERNEL) 2>/dev/null || true
	echo -e "$(yellow)Warning: Kernel $(KERNEL) uninstalled$(normal)"

# ---------------------------------------------------------------------------------------
# SNIPPET pour récupérer les bases de données de nltk.
# Vous pouvez valoriser NLTK_DATA https://www.nltk.org/data.html
ifndef NLTK_DATA
export NTLK_DATA
# The new line slowed the Makefile.
#NLTK_DATA:=$(shell python -c "import nltk.data; print(nltk.data.path[0])" 2>/dev/null || true)
# Else
ifeq ($(OS),Darwin)
ifeq ($(wildcard ~/nltk_data), )
NLTK_DATA:=/usr/local/share/nltk_data
else
NLTK_DATA:=~/nltk_data
endif
else ifeq ($(OS),Windows)
NLTK_DATA:=C:/nltk_data
else
ifeq ($(wildcard ~/nltk_data), )
NLTK_DATA:=/usr/share/nltk_data
else
NLTK_DATA:=~/nltk_data
endif
endif
endif

# Add here the NLTK database.
# For example, add
# NLTK_DATABASE= \
# $(NLTK_DATA)/corpora/wordnet \
# $(NLTK_DATA)/tokenizers/punkt \
# $(NLTK_DATA)/corpora/stopwords
# and that's all.
NLTK_DATABASE=

$(NLTK_DATA)/tokenizers/%: $(PIP_PACKAGE)
	$(VALIDATE_VENV)
	python -O -m nltk.downloader $* || true
	touch $(NLTK_DATA)/tokenizers/$*

$(NLTK_DATA)/corpora/%: $(PIP_PACKAGE)
	$(VALIDATE_VENV)
	python -O -m nltk.downloader $* || true
	touch $(NLTK_DATA)/corpora/$*

# ---------------------------------------------------------------------------------------
# SNIPPET pour récupérer les bases de données de spacy.
# Add here the Spacy database.
# For exemple :
# SPACY_DATABASE= \
#   $(CONDA_PACKAGE)/spacy/data/en
# and that's all.
.PHONY: spacy-database
SPACY_DATABASE=

$(CONDA_PACKAGE)/spacy/data/%: $(PIP_PACKAGE)
	$(VALIDATE_VENV)
	python -O -m spacy download $*
	@touch $(CONDA_PACKAGE)/spacy/data/$*

# ---------------------------------------------------------------------------------------
# SNIPPET pour préparer l'environnement d'un projet juste après un `git clone`
.PHONY: configure
## Prepare the work environment (conda venv, kernel, ...)
configure:
	@conda create --name "$(VENV)" python=$(PYTHON_VERSION) -y
	@if [[ "base" == "$(CONDA_DEFAULT_ENV)" ]] || [[ -z "$(CONDA_DEFAULT_ENV)" ]] ; \
	then echo -e "Use: $(cyan)conda activate $(VENV)$(normal)" ; fi

# ---------------------------------------------------------------------------------------
.PHONY: remove-venv
remove-$(VENV):
	@$(DEACTIVATE_VENV)
	conda env remove --name "$(VENV)" -y
	echo -e "Use: $(cyan)conda deactivate$(normal)"
# Remove venv
remove-venv : remove-$(VENV)

# ---------------------------------------------------------------------------------------
# SNIPPET de mise à jour des dernières versions des composants.
# Après validation, il est nécessaire de modifier les versions dans le fichier `setup.py`
# pour tenir compte des mises à jours
.PHONY: upgrade-venv
upgrade-$(VENV):
	$(VALIDATE_VENV)
	conda update --all
	pip list --format freeze --outdated | sed 's/(.*//g' | xargs -r -n1 pip install $(EXTRA_INDEX) -U
	echo -e "$(cyan)After validation, upgrade the setup.py$(normal)"
# Upgrade packages to last versions
upgrade-venv: upgrade-$(VENV)

# ---------------------------------------------------------------------------------------
# SNIPPET de validation des notebooks en les ré-executants.
# L'idée est d'avoir un sous répertoire par phase, dans le répertoire `notebooks`.
# Ainsi, il suffit d'un `make nb-run-phase1` pour valider tous les notesbooks du répertoire `notebooks/phase1`.
# Pour valider toutes les phases : `make nb-run-*`.
# L'ordre alphabétique est utilisé. Il est conseillé de préfixer chaque notebook d'un numéro.
.PHONY: nb-run-*
notebooks/.make-%: $(REQUIREMENTS) $(JUPYTER_DATA_DIR)/kernels/$(KERNEL)
	$(VALIDATE_VENV)
	time jupyter nbconvert \
	  --ExecutePreprocessor.timeout=-1 \
	  --execute \
	  --inplace notebooks/$*/*.ipynb
	touch notebooks/.make-$*
# All notebooks
notebooks/phases: $(sort $(subst notebooks/,notebooks/.make-,$(wildcard notebooks/*)))

## Invoke all notebooks in lexical order from notebooks/<% dir>
nb-run-%: $(JUPYTER_DATA_DIR)/kernels/$(KERNEL)
	$(MAKE) notebooks/.make-$*


# ---------------------------------------------------------------------------------------
# SNIPPET de validation des scripts en les ré-executant.
# Ces scripts peuvent être la traduction de Notebook Jupyter, via la règle `make nb-convert`.
# L'idée est d'avoir un sous répertoire par phase, dans le répertoire `scripts`.
# Ainsi, il suffit d'un `make run-phase1` pour valider tous les scripts du répertoire `scripts/phase1`.
# Pour valider toutes les phases : `make run-*`.
# L'ordre alphabétique est utilisé. Il est conseillé de préfixer chaque script d'un numéro.
.PHONY: run-*
scripts/.make-%: $(REQUIREMENTS)
	$(VALIDATE_VENV)
	time ls scripts/$*/*.py | grep -v __ | sed 's/\.py//g; s/\//\./g' | \
		xargs -L 1 -t python -O -m
	@touch scripts/.make-$*

# All phases
scripts/phases: $(sort $(subst scripts/,scripts/.make-,$(wildcard scripts/*)))

## Invoke all script in lexical order from scripts/<% dir>
run-%:
	$(MAKE) scripts/.make-$*

# ---------------------------------------------------------------------------------------
# SNIPPET pour valider le code avec pylint et flake8
.PHONY: lint
.pylintrc:
	pylint --generate-rcfile > .pylintrc

## Lint the code
lint: $(REQUIREMENTS) | .pylintrc
	$(VALIDATE_VENV)
	@flake8 bda_project tests || true
	@echo "----------------------"
	@pylint bda_project tests || true


# ---------------------------------------------------------------------------------------
# SNIPPET pour valider le typage avec pytype
$(CONDA_PACKAGE)/bin/pytype:
	@pip install -q pytype

pytype.cfg:
	pytype --generate-config pytype.cfg

.pytype/pyi/bda_project: $(REQUIREMENTS) $(CONDA_PACKAGE)/bin/pytype pytype.cfg
	$(VALIDATE_VENV)
	pytype -V $(PYTHON_VERSION) bda_project
	for phase in scripts/*
	do
	  ( cd $$phase ; find . -type f -name '*.py' -exec pytype -V $(PYTHON_VERSION) {} \; )
	done
	touch .pytype/pyi/bda_project

.PHONY: typing
## Check python typing
typing: .pytype/pyi/bda_project

## Add infered typing in module
add-typing: typing
	find bda_project -type f -name '*.py' -exec merge-pyi -i {} .pytype/pyi/{}i \;
	for phase in scripts/*
	do
	  ( cd $$phase ; find . -type f -name '*.py' -exec merge-pyi -i {} .pytype/pyi/{}i \; )
	done


# ---------------------------------------------------------------------------------------
# SNIPPET pour créer la documentation html et pdf du projet.
# Il est possible d'indiquer build/XXX, ou XXX correspond au type de format
# à produire. Par exemple: html, singlehtml, latexpdf, ...
# Voir https://www.sphinx-doc.org/en/master/usage/builders/index.html
.PHONY: docs
# Use all processors
#PPR SPHINX_FLAGS=-j$(NB_CORE)
SPHINX_FLAGS=
# Generate API docs
docs/source: $(REQUIREMENTS) $(PYTHON_SRC)
	$(VALIDATE_VENV)
	sphinx-apidoc -f -o docs/source bda_project/
	touch docs/source

# Build the documentation in specificed format (build/html, build/latexpdf, ...)
build/%: $(REQUIREMENTS) docs/source docs/* README.md CHANGELOG.md | .git
	@$(VALIDATE_VENV)
	@TARGET=$(*:build/%=%)
	@echo "Build $$TARGET..."
	@LATEXMKOPTS=-silent sphinx-build -M $$TARGET docs build $(SPHINX_FLAGS)
	touch build/$$TARGET

# Build all format of documentations
## Generate and show the HTML documentation
docs: build/html
	$(BROWSER) build/html/index.html

## Watching for changes to compile the docs
servedocs: docs
	watchmedo shell-command -p '*' -c '$(MAKE) build/html' -R -D docs


# ---------------------------------------------------------------------------------------
# SNIPPET pour créer une distribution des sources
.PHONY: sdist
dist/$(PRJ_PACKAGE)-*.tar.gz: $(REQUIREMENTS)
	$(VALIDATE_VENV)
	python setup.py sdist

# Create a source distribution
sdist: dist/$(PRJ_PACKAGE)-*.tar.gz

# ---------------------------------------------------------------------------------------
# SNIPPET pour créer une distribution des binaires au format egg.
# Pour vérifier la version produite :
# python setup.py --version
# Cela correspond au dernier tag d'un format 'version'
.PHONY: bdist
dist/$(subst -,_,$(PRJ_PACKAGE))-*.whl: $(REQUIREMENTS)
	$(VALIDATE_VENV)
	python setup.py bdist_wheel

# Create a binary wheel distribution
bdist: dist/$(subst -,_,$(PRJ_PACKAGE))-*.whl


# ---------------------------------------------------------------------------------------
# SNIPPET pour créer une distribution des binaires au format egg.
# Pour vérifier la version produite :
# python setup.py --version
# Cela correspond au dernier tag d'un format 'version'
.PHONY: dist

## Create a full distribution
dist: bdist sdist



# ---------------------------------------------------------------------------------------
# SNIPPET pour executer jupyter notebook, mais en s'assurant de la bonne application des dépendances.
# Utilisez `make notebook` à la place de `jupyter notebook`.
.PHONY: notebook
## Start jupyter notebooks
notebook: $(REQUIREMENTS) $(JUPYTER_DATA_DIR)/kernels/$(KERNEL)
	$(VALIDATE_VENV)
	DATA=$$DATA jupyter notebook --ExecutePreprocessor.kernel_name=$(KERNEL) 





# ---------------------------------------------------------------------------------------
# SNIPPET pour convertir tous les notebooks du repertoire 'notebooks' en script
# python dans le répertoire 'src/scripts', déjà compatible avec le mode scientifique de PyCharm Pro.
# Le code utilise un modèle permettant d'encadrer les cellules Markdown dans des strings.
# Les scripts possède ensuite le flag d'exécution, pour pouvoir les lancer directement
# via un 'scripts/phase1/1_sample.py'.
.PHONY: nb-convert
# Convert all notebooks to python scripts
_nbconvert:  $(JUPYTER_DATA_DIR)/kernels/$(KERNEL)
	@echo -e "Convert all notebooks..."
	notebook_path=notebooks
	script_path=scripts
	tmpfile=$$(mktemp /tmp/make-XXXXX)
	
	cat >$${tmpfile} <<TEMPLATE
	{% extends 'python.tpl' %}
	{% block in_prompt %}# %%{% endblock in_prompt %}
	{%- block header -%}
	#!/usr/bin/env python
	# coding: utf-8
	{% endblock header %}
	{% block input %}
	{{ cell.source | ipython2python }}{% endblock input %}
	{% block markdowncell scoped %}
	# %% md
	"""
	{{ cell.source  }}
	"""
	{% endblock markdowncell %}
	TEMPLATE
	
	while IFS= read -r -d '' filename; do
		target=$$(echo $$filename | sed "s/^$${notebook_path}/$${script_path}/g; s/ipynb$$/py/g ; s/[ -]/_/g" )
		mkdir -p $$(dirname $${target})
		jupyter nbconvert --to python --ExecutePreprocessor.kernel_name=$(KERNEL) \
		  --template=$${tmpfile} --stdout "$${filename}" >"$${target}"
		chmod +x $${target}
		@echo -e "Convert $${filename} to $${target}"
	done < <(find notebooks -name '*.ipynb' -type f -not -path '*/\.*' -prune -print0)
	echo -e "$(cyan)All new scripts are in $${target}$(normal)"

# Version permettant de convertir les notebooks et de la ajouter en même temps à GIT
# en ajouter le flag +x.
## Convert all notebooks to python scripts
nb-convert: _nbconvert
	find scripts/ -type f -iname "*.py" -exec git add "{}" \;
	find scripts/ -type f -iname "*.py" -exec git update-index --chmod=+x  "{}" \;

# ---------------------------------------------------------------------------------------
# SNIPPET pour nettoyer tous les fichiers générés par le compilateur Python.
.PHONY: clean-pyc
# Clean pre-compiled files
clean-pyc:
	-/usr/bin/find . -type f -name "*.py[co]" -delete
	-/usr/bin/find . -type d -name "__pycache__" -delete

# ---------------------------------------------------------------------------------------
# SNIPPET pour nettoyer les fichiers de builds (package et docs).
.PHONY: clean-build
# Remove build artifacts and docs
clean-build:
	@-/usr/bin/find . -type f -name ".make-*" -delete
	@rm -fr build/
	@rm -fr dist/*
	@rm -fr *.egg-info

# ---------------------------------------------------------------------------------------
# SNIPPET pour nettoyer tous les notebooks
.PHONY: clean-notebooks
## Remove all results of notebooks
clean-notebooks:
	@[ -e notebooks ] && find notebooks -name '*.ipynb' -exec jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace {} \;
	@echo "Notebooks cleaned"

# ---------------------------------------------------------------------------------------
.PHONY: clean-pip
# Remove all the pip package
clean-pip:
	$(VALIDATE_VENV)
	pip freeze | grep -v "^-e" | xargs pip uninstall -y

# ---------------------------------------------------------------------------------------
# SNIPPET pour nettoyer complètement l'environnement Conda
.PHONY: clean-venv clean-$(VENV)
clean-$(VENV): remove-venv
	@echo -e "$(cyan)Re-create virtualenv $(VENV)...$(normal)"
	@conda create -y -q -n $(VENV)
	@touch setup.py
	@echo -e "$(yellow)Warning: Conda virtualenv $(VENV) is empty.$(normal)"
# Set the current VENV empty
clean-venv : clean-$(VENV)

# ---------------------------------------------------------------------------------------
# SNIPPET pour faire le ménage du projet (hors environnement)
.PHONY: clean
## Clean current environment
clean: clean-pyc clean-build clean-notebooks
	find . -type f -name ".make-*" -delete

# ---------------------------------------------------------------------------------------
# SNIPPET pour faire le ménage du projet
.PHONY: clean-all
# Clean all environments
clean-all: clean remove-venv remove-kernel

# ---------------------------------------------------------------------------------------
# SNIPPET pour executer les tests unitaires et les tests fonctionnels
# Voir https://setuptools.readthedocs.io/en/latest/setuptools.html#test-build-package-and-run-a-unittest-suite
PYTEST_PARALLEL ?=-n $(NB_CORE)
.PHONY: test unittest functionaltest
.make-unit-test: $(REQUIREMENTS) $(PYTHON_TST) $(PYTHON_SRC)
	$(VALIDATE_VENV)
	#python setup.py test
	python -m pytest  -s tests $(PYTEST_PARALLEL) -m "not functional"
	touch .make-unit-test
# Run only unit tests
unit-test: .make-unit-test

.make-functional-test: $(REQUIREMENTS) $(PYTHON_TST) $(PYTHON_SRC)
	$(VALIDATE_VENV)
	python -m pytest  -s tests $(PYTEST_PARALLEL) -m "functional"
	touch .make-functional-test
# Run only functional tests
functional-test: .make-functional-test

## Run all tests (unit and functional)
.make-test: $(REQUIREMENTS) $(PYTHON_TST) $(PYTHON_SRC)
	# python setup.py test
	python -m pytest  -s tests $(PYTEST_PARALLEL)
	touch .make-test .make-unit-test .make-functional-test
test: .make-test


# SNIPPET pour vérifier les TU et le recalcule de tous les notebooks et scripts.
# Il peut etre judicieux d'ajouter lint et typing, pour valider le respect des normes Pythons
.PHONY: validate
## Validate the version before release
validate: .make-test $(DATA)/raw scripts/* notebooks/* build/html build/linkcheck






# ---------------------------------------------------------------------------------------
# Function to invoke DVC run
# $1 : dvc file
# $2 : cmd to invoke (python ...)
# $3 : empty or -M
define dvc_run
	if [ -e $1 ] ; then dvc remove $1 ; fi
	rm -f $1
	#dvc remove $(addsuffix .dvc,$@)
	#	--no-commit
	dvc run -f $1 \
		$(addprefix -d ,$(filter-out .%,$?)) \
		$(if $3,-M $@,-o $@) \
	$2
	if [ -e $(@D)/.gitignore ] ; then git add $(@D)/.gitignore ; fi
	if [ -e $1 ] ; then git add $1 ; fi
endef


# ---------------------------------------------------------------------------------------
# Initialize DVC
.dvc: | .git
	dvc init
	dvc config cache.protected true
	dvc remote add -d origin $(DVC_BUCKET)
	git add .dvc/config
REQUIREMENTS += | .dvc

# ---------------------------------------------------------------------------------------
# SNIPPET pour initialiser DVC avec la production de fichiers distants dans des buckets
# Voir: https://dvc.org/doc/user-guide/external-outputs
# Use :
# - make dvc-external-s3cache
# - make dvc-external-gscache
# - make dvc-external-azurecache
# - make dvc-external-sshcache
# - make dvc-external-htfscache
.PHONY: dvc-external-*
## Initialize the DVC external cache provider (dvc-external-s3cache)
dvc-external-%: .dvc
	dvc remote add ${*} $(BUCKET)/cache
	dvc config cache.gs $*

# ---------------------------------------------------------------------------------------
# SNIPPET pour vérouiller un fichier DVC pour ne plus le reconstruire, meme si cela
# semble nécessaire. C'est util en phase de développement.
# See https://dvc.org/doc/commands-reference/lock
# Lock DVC file
lock-%: .dvc
	dvc lock $(*:lock-%=%)

# ---------------------------------------------------------------------------------------
# SNIPPET pour afficher les métrics gérées par DVC.
# See https://dvc.org/doc/commands-reference/metrics
## show the DVC metrics
metrics: .dvc
	dvc metrics show

# ---------------------------------------------------------------------------------------
# SNIPPET pour supprimer les fichiers de DVC du projet
# Remove all .dvc files
clean-dvc:
	rm -Rf .dvc
	-/usr/bin/find . -type f -name "*.dvc" -delete

#################################################################################
# PROJECT RULES                                                                 #
#################################################################################
#
# ┌─────────┐ ┌──────────┐ ┌───────┐ ┌──────────┐ ┌───────────┐
# │ prepare ├─┤ features ├─┤ train ├─┤ evaluate ├─┤ visualize │
# └─────────┘ └──────────┘ └───────┘ └──────────┘ └───────────┘
#

TOOLS=$(shell find bda_project/ -mindepth 2 -type f -name '*.py')
.PHONY: prepare features train evaluate visualize

# Rule to declare an implicite dependencies from sub module for all root project files
bda_project/*.py : $(TOOLS)
	@touch $@

data/interim/datas-prepared.csv: $(REQUIREMENTS) bda_project/prepare_dataset.py data/raw/*
	$(call dvc_run,prepare.dvc,\
	python -O -m bda_project.prepare_dataset \
		data/raw/datas.csv \
		data/interim/datas-prepared.csv\
	)
prepare.dvc: data/interim/datas-prepared.csv
## Prepare the dataset
prepare: prepare.dvc

data/interim/datas-features.csv : $(REQUIREMENTS) bda_project/build_features.py data/interim/datas-prepared.csv
	$(call dvc_run,feature.dvc,\
	python -O -m bda_project.build_features \
		data/interim/datas-prepared.csv \
		data/interim/datas-features.csv\
	)
## Add features
features: data/interim/datas-features.csv

models/model.pkl : $(REQUIREMENTS) bda_project/train_model.py data/interim/datas-features.csv
	$(call dvc_run,train.dvc,\
	python -O -m bda_project.train_model \
		data/interim/datas-features.csv \
		models/model.pkl \
	)
## Train the model
train: models/model.pkl


reports/auc.metric: $(REQUIREMENTS) bda_project/evaluate_model.py models/model.pkl
	$(call dvc_run,evaluate.dvc,\
	python -O -m bda_project.evaluate_model \
		models/model.pkl \
		data/interim/datas-features.csv \
		reports/auc.metric \
	,-M \
	)
## Evalutate the model
evaluate: reports/auc.metric

## Visualize the result
visualize: $(REQUIREMENTS) bda_project/visualize.py models/model.pkl
	python -O -m bda_project.visualize \
	    reports/


# See https://dvc.org/doc/commands-reference/repro
.PHONY: repro
## 	Re-run commands recorded in the last DVC stages in the same order.
repro: evaluate.dvc
	dvc repro $<



